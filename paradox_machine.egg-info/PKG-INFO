Metadata-Version: 2.1
Name: paradox-machine
Version: 0.1.0
Summary: A structured paradox detection toolkit built on multi-phase LLM reasoning.
Author: Paradox Machine Contributors
Keywords: llm,paradox,reasoning,analysis,prompt-engineering
Classifier: Development Status :: 3 - Alpha
Classifier: Intended Audience :: Science/Research
Classifier: Intended Audience :: Developers
Classifier: Programming Language :: Python :: 3
Classifier: Programming Language :: Python :: 3.10
Classifier: Programming Language :: Python :: 3.11
Classifier: Programming Language :: Python :: 3.12
Classifier: Programming Language :: Python :: 3.13
Classifier: Topic :: Scientific/Engineering :: Artificial Intelligence
Requires-Python: >=3.10
Description-Content-Type: text/markdown
Requires-Dist: PyYAML>=6.0
Provides-Extra: dev
Requires-Dist: pytest>=8.0; extra == "dev"
Requires-Dist: ruff>=0.5; extra == "dev"

# Paradox Machine

A minimum viable tool (MVP) for detecting paradoxes in viewpoints and proposals.

The core goal is to move an LLM from default agreement mode into strict logical evaluation mode, then output a structured paradox report to surface self-contradictions, hidden fallacies, and costly trade-offs earlier.

## What It Does

- Structured multi-step analysis (S1 + Phase I/II/III)
- Standardized `Paradox Report` output (human-readable text or JSON)
- Model switching via YAML configuration
- A direct Q&A mode (`src/demo.py`) for A/B comparison against the structured pipeline

## Project Layout

```text
.
├── run.py                    # Structured paradox detection CLI entry
├── src/
│   ├── agents.py             # Main agent pipeline + API client
│   ├── prompts.py            # All prompt templates
│   ├── apis.py               # YAML model config loader
│   └── demo.py               # Direct Q&A mode (for comparison)
├── assets/models/
│   ├── deepseek-chat.yaml
│   └── deepseek-reasoner.yaml
└── docs/
    ├── intro.md
    └── full_process.md
```

## Quick Start

### 1) Install

```bash
python -m venv .venv
source .venv/bin/activate
pip install -e .
```

### 2) Configure Model API

Configure model settings in `assets/models/*.yaml` (DeepSeek examples are included):

```yaml
provider: deepseek
model: deepseek-chat
base_url: https://api.deepseek.com
chat_completions_path: /chat/completions
api_key_env: DEEPSEEK_API_KEY
api_key: ""
timeout_seconds: 90
default_temperature: 0.2
headers:
  Content-Type: application/json
```

If `assets/models/*.yaml` is missing locally, create the file manually and fill in the fields above.

## Usage

### A) Structured Paradox Detection

```bash
python run.py --config deepseek-chat "To reduce latency, we plan to add unlimited cache layers and minimize consistency checks."
```

JSON output:

```bash
python run.py --config deepseek-chat --json "To reduce latency, we plan to add unlimited cache layers and minimize consistency checks."
```

Switch model:

```bash
python run.py --config deepseek-reasoner "Your proposal here"
```

### B) Direct Q&A (For A/B Comparison)

Single-turn Q&A:

```bash
python -m src.demo --config deepseek-chat "To reduce latency, we plan to add unlimited cache layers and minimize consistency checks."
```

Interactive mode:

```bash
python -m src.demo --config deepseek-chat --interactive
```

Suggested A/B comparison flow:

1. Run `python -m src.demo ...` to observe a natural model response.
2. Run `python run.py ...` to generate a structured paradox report.
3. Compare rebuttal strength, structure quality, and actionable guidance.

## Model Config Selection Rules

`--config` supports two formats:

- Config name: `deepseek-chat`, `deepseek-reasoner`
- Config path: `assets/models/deepseek-chat.yaml`

If `--config` is omitted:

- First try env var `PARADOX_MODEL_CONFIG`
- Otherwise default to `assets/models/deepseek-chat.yaml`

## Common Errors

`API key is empty ...`

- `DEEPSEEK_API_KEY` is not set
- Or `api_key_env` / `api_key` is not configured correctly in YAML

`Model config not found ...`

- `--config` name/path is wrong
- Check filenames under `assets/models/`

`response is not in expected format ...`

- Model response is not OpenAI-compatible `chat/completions` format
- Verify `base_url`, `chat_completions_path`, and provider compatibility

## References

- Motivation: `docs/intro.md`
- Method pipeline: `docs/full_process.md`
